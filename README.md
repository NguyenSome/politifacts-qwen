# Qwen2.5 Fact-Checking Classifier (Politifact) ğŸ›°ï¸

Pipeline for fine-tuning Qwen2.5 on the [Politifact] fact-checking dataset, with:

- Config-driven training (YAML)
- QLoRA fine-tuning on GPU-constrained hardware
- MLflow experiment tracking
- Dockerized training & evaluation
- AWS-ready (EC2 + ECR) workflow

This repo is structured to showcase a workflow from dataset â†’ training â†’ evaluation â†’ experiment tracking â†’ packaged Docker
image that can run locally or in the cloud.

---

## Project Overview

- **Goal**: Classify political statements into discrete truthfulness labels  
  (e.g. `pants-fire`, `false`, `mostly-false`, `half-true`, `mostly-true`, `true`).
- **Model**: Qwen2.5-0.5B fine-tuned with QLoRA.
- **Stack**:
  - Python 3.11
  - PyTorch + Transformers
  - PEFT / QLoRA
  - MLflow (local tracking via `mlflow.db` or remote tracking URI)
  - Docker + NVIDIA CUDA 12.9
  - (Optional) AWS EC2 + ECR for training in the cloud

---

## Repository Structure

â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base.yaml         # Base experiment configuration
â”‚   â”œâ”€â”€ test.yaml         # Script testing configuration
â”œâ”€â”€ data/                 # Postprocessed Politifacts data obtained from Kaggle
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ finetune.py          # Training entrypoint (QLoRA fine-tuning, MLflow logging)
â”‚   â”œâ”€â”€ zero_shot_eval.py    # Baseline model testing (metrics, confusion matrix, etc.)
â”œâ”€â”€ notebooks/            # (not committed) Exploration / EDA / debugging 
â”œâ”€â”€ scripts/              # (not committed) Exploration / debugging
â”œâ”€â”€ models/               # (not committed) saved model checkpoints
â”œâ”€â”€ results/              # (not committed) evaluation artifacts
â”œâ”€â”€ logs/                 # (not committed) log files
â”œâ”€â”€ mlruns/               # (not committed) mlflow logs and artifacts
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Makefile
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md