{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c85e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gc\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score, precision_recall_fscore_support\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback,\n",
    "    Trainer,\n",
    "    TrainerCallback,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e68df76",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\n",
    "    \"pants-fire\",\n",
    "    \"false\",\n",
    "    \"mostly-false\",\n",
    "    \"half-true\",\n",
    "    \"mostly-true\",\n",
    "    \"true\",\n",
    "]\n",
    "\n",
    "bf16_ok = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "fp16_ok = torch.cuda.is_available()\n",
    "dtype = torch.bfloat16 if bf16_ok else (torch.float16 if fp16_ok else torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc85c69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Classify the following statement with one label only from: pants-fire, false, mostly-false, half-true, mostly-true, true.\\n Statement: {statement}\\n Answer with only the label, nothing else:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "258dc0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e968067274b457387abb4894d08e082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = \"Qwen/Qwen2.5-3B\"\n",
    "\n",
    "# Qlora configuration - paper ref\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=dtype,\n",
    ")\n",
    "\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    trust_remote_code=True,\n",
    "    dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_cfg,\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"sdpa\",\n",
    ")\n",
    "\n",
    "base.config.use_cache = False\n",
    "base = prepare_model_for_kbit_training(base, use_gradient_checkpointing=True)\n",
    "\n",
    "target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\",\n",
    "]\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=target_modules,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    bias=\"none\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model, use_fast=True, trust_remote_code=True\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e809bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokenize_fn(tokenizer, max_length: int):\n",
    "\n",
    "    def tokenize_batch(batch: dict[str, list[Any]]) -> dict[str, list[list[int]]]:\n",
    "        statements: list[str] = [str(s).strip() for s in batch[\"statement\"]]\n",
    "        verdicts: list[str] = [str(v).strip() for v in batch[\"verdict\"]]\n",
    "\n",
    "        input_ids_all: list[list[int]] = []\n",
    "        label_all: list[list[int]] = []\n",
    "\n",
    "        orig_side = tokenizer.truncation_side\n",
    "        tokenizer.truncation_side = \"right\"\n",
    "\n",
    "        for s, v in zip(statements, verdicts, strict=False):\n",
    "            text = prompt.format(statement=s)\n",
    "            target = \" \" + v\n",
    "\n",
    "            target_ids = tokenizer(target, add_special_tokens=False)[\"input_ids\"]\n",
    "            available = max(0, max_length - len(target_ids))\n",
    "            prompt_ids = tokenizer(\n",
    "                text,\n",
    "                add_special_tokens=False,\n",
    "                truncation=True,\n",
    "                max_length=available,\n",
    "            )[\"input_ids\"]\n",
    "\n",
    "            ids = prompt_ids + target_ids\n",
    "            lbl = ([-100] * len(prompt_ids)) + target_ids\n",
    "\n",
    "            input_ids_all.append(ids)\n",
    "            label_all.append(lbl)\n",
    "\n",
    "        tokenizer.truncation_side = orig_side\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids_all,\n",
    "            \"labels\": label_all,\n",
    "            \"verdict\": verdicts,\n",
    "            \"statement\": statements,\n",
    "        }\n",
    "\n",
    "    return tokenize_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2fe5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(cfgs, tokenizer, split_name: str = \"training\"):\n",
    "    data_path = cfgs[\"data\"][split_name]\n",
    "    ds = load_dataset(\"json\",\n",
    "                      data_files=data_path,\n",
    "                      keep_in_memory=False,)\n",
    "\n",
    "    max_length = 1024\n",
    "    tokenize_fn = build_tokenize_fn(\n",
    "        tokenizer, max_length,\n",
    "        )\n",
    "\n",
    "    if split_name == \"training\":\n",
    "        split = ds[\"train\"].train_test_split(test_size=0.15, seed=cfgs.get(\"seed\", 7))\n",
    "        dataset = DatasetDict(\n",
    "            {\n",
    "                \"train\": split[\"train\"],\n",
    "                \"validation\": split[\"test\"],\n",
    "            }\n",
    "        )\n",
    "        cols_to_drop = dataset[\"train\"].column_names\n",
    "\n",
    "        def tokenize_for_train(batch: dict[str, list[Any]]) -> dict[str, Any]:\n",
    "            out = tokenize_fn(batch)\n",
    "            # We don't need these during training/validation; they only cost RAM.\n",
    "            out.pop(\"statement\", None)\n",
    "            out.pop(\"verdict\", None)\n",
    "            return out\n",
    "\n",
    "        return dataset.map(\n",
    "            tokenize_for_train,\n",
    "            batched=True,\n",
    "            batch_size=32,\n",
    "            num_proc=1,\n",
    "            remove_columns=cols_to_drop,\n",
    "            desc=\"Tokenizing (train/val)\",\n",
    "        )\n",
    "    else:\n",
    "        cols_to_drop = ds[\"train\"].column_names\n",
    "        # tokenize_fn returns \"statement\" and \"verdict\", so we can remove the original columns\n",
    "        # Access the \"train\" split before mapping since load_dataset returns DatasetDict with \"train\" key\n",
    "        tokenized = ds[\"train\"].map(\n",
    "            tokenize_fn,\n",
    "            batched=True,\n",
    "            batch_size=32,\n",
    "            num_proc=1,\n",
    "            remove_columns=cols_to_drop,\n",
    "            desc=f\"Tokenizing ({split_name})\",\n",
    "        )\n",
    "\n",
    "        return DatasetDict({split_name: tokenized})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa0527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.config\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\", message=\"The following generation flags are not valid\"\n",
    ")\n",
    "\n",
    "class MemUsageCallback(TrainerCallback):\n",
    "    def _log_mem(self, where: str):\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "        torch.cuda.synchronize()\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved  = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(\n",
    "            f\"[{where}] [GPU {torch.cuda.current_device()}] \"\n",
    "            f\"Allocated: {allocated:.2f} GB | Reserved: {reserved:.2f} GB\",\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        self._log_mem(\"train_step_end\")\n",
    "        gc.collect()\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        # Called once per eval loop\n",
    "        self._log_mem(\"on_evaluate_start\")\n",
    "        gc.collect()\n",
    "\n",
    "LOGGER = logging.getLogger(\"my_app\")\n",
    "\n",
    "LABELS = [\n",
    "    \"pants-fire\",\n",
    "    \"false\",\n",
    "    \"mostly-false\",\n",
    "    \"half-true\",\n",
    "    \"mostly-true\",\n",
    "    \"true\",\n",
    "]\n",
    "\n",
    "bf16_ok = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "fp16_ok = torch.cuda.is_available()\n",
    "dtype = torch.bfloat16 if bf16_ok else (torch.float16 if fp16_ok else torch.float32)\n",
    "\n",
    "\n",
    "def setup_logging() -> None:\n",
    "    config_file = Path(\"configs/config.json\")\n",
    "    if config_file.exists():\n",
    "        with config_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            config = json.load(f)\n",
    "        logging.config.dictConfig(config)\n",
    "    else:\n",
    "        logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "\n",
    "def _read_config(path: str | Path = \"configs/base.yaml\") -> dict[str, Any]:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Config file not found: {p}\")\n",
    "\n",
    "    try:\n",
    "        with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            cfg = yaml.safe_load(f) or {}\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Invalid YAML in {p}: {e}\") from e\n",
    "\n",
    "    cfg.setdefault(\"data\", {})\n",
    "    cfg.setdefault(\"model\", {})\n",
    "    cfg.setdefault(\"lora\", {})\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def load_model(cfg: dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Parameters for model, tokenizer, and QLoRa training.\n",
    "    Only a few parameters are added to configuration setting to spec as needed.\n",
    "    \"\"\"\n",
    "    LOGGER.info(\"Loading base model %s\", cfg[\"model\"].get(\"model_name\", \"<missing>\"))\n",
    "\n",
    "    base_model = \"Qwen/Qwen2.5-3B\"\n",
    "\n",
    "    # Qlora configuration - paper ref\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=dtype,\n",
    "    )\n",
    "\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        trust_remote_code=True,\n",
    "        dtype=dtype,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        low_cpu_mem_usage=True,\n",
    "        attn_implementation=\"sdpa\",\n",
    "    )\n",
    "\n",
    "    base.config.use_cache = False\n",
    "    base = prepare_model_for_kbit_training(base, use_gradient_checkpointing=True)\n",
    "\n",
    "    target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ]\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=target_modules,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        bias=\"none\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model, use_fast=True, trust_remote_code=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def build_tokenize_fn(tokenizer, prompt: str, max_length: int):\n",
    "    \"\"\"\n",
    "    Tokenize per-example, truncating the prompt to leave room for the target.\n",
    "    Returns variable-length lists; collator will handle padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize_batch(batch: dict[str, list[Any]]) -> dict[str, list[list[int]]]:\n",
    "        statements: list[str] = [str(s).strip() for s in batch[\"statement\"]]\n",
    "        verdicts: list[str] = [str(v).strip() for v in batch[\"verdict\"]]\n",
    "\n",
    "        input_ids_all: list[list[int]] = []\n",
    "        label_all: list[list[int]] = []\n",
    "\n",
    "        orig_side = tokenizer.truncation_side\n",
    "        tokenizer.truncation_side = \"right\"\n",
    "\n",
    "        for s, v in zip(statements, verdicts, strict=False):\n",
    "            text = prompt.format(statement=s)\n",
    "            target = \" \" + v\n",
    "\n",
    "            target_ids = tokenizer(target, add_special_tokens=False)[\"input_ids\"]\n",
    "            available = max(0, max_length - len(target_ids))\n",
    "            prompt_ids = tokenizer(\n",
    "                text,\n",
    "                add_special_tokens=False,\n",
    "                truncation=True,\n",
    "                max_length=available,\n",
    "            )[\"input_ids\"]\n",
    "\n",
    "            ids = prompt_ids + target_ids\n",
    "            lbl = ([-100] * len(prompt_ids)) + target_ids\n",
    "\n",
    "            input_ids_all.append(ids)\n",
    "            label_all.append(lbl)\n",
    "\n",
    "        tokenizer.truncation_side = orig_side\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids_all,\n",
    "            \"labels\": label_all,\n",
    "            \"verdict\": verdicts,\n",
    "            \"statement\": statements,\n",
    "        }\n",
    "\n",
    "    return tokenize_batch\n",
    "\n",
    "\n",
    "def data_processing(cfgs, tokenizer, split_name: str = \"training\"):\n",
    "    data_path = cfgs[\"data\"][split_name]\n",
    "    ds = load_dataset(\"json\",\n",
    "                      data_files=data_path,\n",
    "                      keep_in_memory=False,)\n",
    "\n",
    "    max_length = int(cfgs[\"data\"].get(\"max_length\", 1024))\n",
    "    tokenize_fn = build_tokenize_fn(\n",
    "        tokenizer, cfgs[\"prompt\"][\"template\"], max_length,\n",
    "        )\n",
    "\n",
    "    if split_name == \"training\":\n",
    "        split = ds[\"train\"].train_test_split(test_size=0.15, seed=cfgs.get(\"seed\", 7))\n",
    "        dataset = DatasetDict(\n",
    "            {\n",
    "                \"train\": split[\"train\"],\n",
    "                \"validation\": split[\"test\"],\n",
    "            }\n",
    "        )\n",
    "        cols_to_drop = dataset[\"train\"].column_names\n",
    "\n",
    "        def tokenize_for_train(batch: dict[str, list[Any]]) -> dict[str, Any]:\n",
    "            out = tokenize_fn(batch)\n",
    "            # We don't need these during training/validation; they only cost RAM.\n",
    "            out.pop(\"statement\", None)\n",
    "            out.pop(\"verdict\", None)\n",
    "            return out\n",
    "\n",
    "        return dataset.map(\n",
    "            tokenize_for_train,\n",
    "            batched=True,\n",
    "            batch_size=32,\n",
    "            num_proc=1,\n",
    "            remove_columns=cols_to_drop,\n",
    "            desc=\"Tokenizing (train/val)\",\n",
    "        )\n",
    "    else:\n",
    "        cols_to_drop = ds[\"train\"].column_names\n",
    "        # tokenize_fn returns \"statement\" and \"verdict\", so we can remove the original columns\n",
    "        # Access the \"train\" split before mapping since load_dataset returns DatasetDict with \"train\" key\n",
    "        tokenized = ds[\"train\"].map(\n",
    "            tokenize_fn,\n",
    "            batched=True,\n",
    "            batch_size=32,\n",
    "            num_proc=1,\n",
    "            remove_columns=cols_to_drop,\n",
    "            desc=f\"Tokenizing ({split_name})\",\n",
    "        )\n",
    "\n",
    "        return DatasetDict({split_name: tokenized})\n",
    "\n",
    "\n",
    "def build_metrics_computer(tokenizer):\n",
    "    \"\"\"\n",
    "    Exact-string accuracy for the target only. Convert from logits -> ids when needed and compare on the label positions (mask != -100).\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        if preds.ndim == 3:\n",
    "            pred_ids = preds.argmax(-1)\n",
    "        else:\n",
    "            pred_ids = preds\n",
    "\n",
    "        labels = np.array(labels)\n",
    "        pred_ids = np.array(pred_ids)\n",
    "\n",
    "        y_true: list[str] = []\n",
    "        y_pred: list[str] = []\n",
    "\n",
    "        for i in range(labels.shape[0]):\n",
    "            label_row = labels[i]\n",
    "            pred_row = pred_ids[i]\n",
    "\n",
    "            shifted_labels = label_row[1:]\n",
    "            shifted_preds = pred_row[:-1]\n",
    "\n",
    "            mask = shifted_labels != -100\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "\n",
    "            true_ids = shifted_labels[mask]\n",
    "            pred_span_ids = shifted_preds[mask]\n",
    "            true_text = (\n",
    "                tokenizer.decode(true_ids, skip_special_tokens=True).strip().lower()\n",
    "            )\n",
    "            pred_text = (\n",
    "                tokenizer.decode(pred_span_ids, skip_special_tokens=True)\n",
    "                .strip()\n",
    "                .lower()\n",
    "            )\n",
    "\n",
    "            y_true.append(true_text)\n",
    "            y_pred.append(pred_text)\n",
    "\n",
    "        accuracy = (\n",
    "            float((np.array(y_pred) == np.array(y_true)).mean()) if y_true else 0.0\n",
    "        )\n",
    "\n",
    "        valid_idx = [i for i, p in enumerate(y_pred) if p in LABELS]\n",
    "        if valid_idx:\n",
    "            vt = [y_true[i] for i in valid_idx]\n",
    "            vp = [y_pred[i] for i in valid_idx]\n",
    "\n",
    "            macro_f1 = f1_score(vt, vp, labels=LABELS, average=\"macro\", zero_division=0)\n",
    "            kappa = cohen_kappa_score(vt, vp, labels=LABELS)\n",
    "            recognized_ratio = len(valid_idx) / len(y_true)\n",
    "\n",
    "            prec, rec, f1, supp = precision_recall_fscore_support(\n",
    "                vt, vp, labels=LABELS, zero_division=0\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"macro_f1\": float(macro_f1),\n",
    "                \"cohen_kappa\": float(kappa),\n",
    "                \"recognized_ratio\": float(recognized_ratio),\n",
    "            }\n",
    "\n",
    "            for i, lab in enumerate(LABELS):\n",
    "                metrics[f\"per_class/precision/{lab}\"] = float(prec[i])\n",
    "                metrics[f\"per_class/recall/{lab}\"] = float(rec[i])\n",
    "                metrics[f\"per_class/f1/{lab}\"] = float(f1[i])\n",
    "                metrics[f\"per_class/support/{lab}\"] = float(supp[i])\n",
    "\n",
    "        else:\n",
    "            metrics = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"macro_f1\": 0.0,\n",
    "                \"cohen_kappa\": 0.0,\n",
    "                \"recognized_ratio\": 0.0,\n",
    "            }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "def write_json(metrics: dict[str, float], path: Path) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(metrics, file, indent=2)\n",
    "    LOGGER.info(\"Persisted metrics to %s\", path)\n",
    "\n",
    "\n",
    "def _manual_training_params() -> dict[str, Any]:\n",
    "    \"\"\"Single place to tweak core training loop knobs.\"\"\"\n",
    "\n",
    "    return {\n",
    "            \"output_dir\": Path(\"results/ar-qwen-mini\").resolve(),\n",
    "            \"logging_steps\": 50,   # how often to log training loss\n",
    "        }\n",
    "\n",
    "\n",
    "def train(model, tokenizer, cfgs: dict[str, Any], training_runtime: dict[str, Any]):\n",
    "\n",
    "    dataset = data_processing(cfgs, tokenizer, \"training\")\n",
    "    compute_metrics = build_metrics_computer(tokenizer)\n",
    "    collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer, label_pad_token_id=-100, pad_to_multiple_of=8\n",
    "    )\n",
    "\n",
    "    out_dir = training_runtime[\"output_dir\"]\n",
    "    log_every_steps = training_runtime[\"logging_steps\"]\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(out_dir),\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.1,\n",
    "\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        logging_steps=log_every_steps,\n",
    "        eval_accumulation_steps=1,\n",
    "\n",
    "        tf32=True,\n",
    "        bf16=bf16_ok,\n",
    "        fp16=(fp16_ok and not bf16_ok),\n",
    "        max_grad_norm=0.3,\n",
    "        dataloader_num_workers=0,\n",
    "        optim=\"adamw_bnb_8bit\",\n",
    "        save_safetensors=True,\n",
    "\n",
    "        report_to=[],\n",
    "\n",
    "        remove_unused_columns=True,\n",
    "        seed=cfgs[\"seed\"],\n",
    "\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        greater_is_better=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=collator,\n",
    "        args=args,\n",
    "        callbacks=[MemUsageCallback()],\n",
    "    )\n",
    "\n",
    "    trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))\n",
    "\n",
    "    LOGGER.info(\n",
    "        \"Starting training run (epochs=%s, lr=%s)\",\n",
    "        args.num_train_epochs,\n",
    "        args.learning_rate,\n",
    "    )\n",
    "\n",
    "    train_output = trainer.train()\n",
    "    train_metrics = train_output.metrics or {}\n",
    "    eval_metrics = trainer.evaluate()  # includes eval_loss + compute_metrics()\n",
    "\n",
    "    numeric_metrics = {\n",
    "        k: float(v)\n",
    "        for k, v in {**train_metrics, **eval_metrics}.items()\n",
    "        if isinstance(v, (int, float))\n",
    "    }\n",
    "\n",
    "    if not numeric_metrics:\n",
    "        LOGGER.warning(\"No numeric metrics returned from trainer run.\")\n",
    "    else:\n",
    "        LOGGER.info(\"Final metrics (also written to training_metrics.json):\")\n",
    "        for key in sorted(numeric_metrics):\n",
    "            LOGGER.info(\"  %s = %.6f\", key, numeric_metrics[key])\n",
    "\n",
    "    write_json(numeric_metrics, out_dir / \"training_metrics.json\")\n",
    "\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config\", default=\"configs/base.yaml\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    setup_logging()\n",
    "    LOGGER.info(\"Logging setup complete\")\n",
    "    CONFIG_DIR = Path(args.config).resolve()\n",
    "    cfgs = _read_config(CONFIG_DIR)\n",
    "    model, tokenizer = load_model(cfgs)\n",
    "    training_runtime = _manual_training_params()\n",
    "    train(model, tokenizer, cfgs, training_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38ef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.config\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainerCallback,\n",
    ")\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\n",
    "    action=\"ignore\", message=\"The following generation flags are not valid\"\n",
    ")\n",
    "\n",
    "class MemUsageCallback(TrainerCallback):\n",
    "    def _log_mem(self, where: str):\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "        torch.cuda.synchronize()\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved  = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(\n",
    "            f\"[{where}] [GPU {torch.cuda.current_device()}] \"\n",
    "            f\"Allocated: {allocated:.2f} GB | Reserved: {reserved:.2f} GB\",\n",
    "            flush=True,\n",
    "        )\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        self._log_mem(\"train_step_end\")\n",
    "        gc.collect()\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        # Called once per eval loop\n",
    "        self._log_mem(\"on_evaluate_start\")\n",
    "        gc.collect()\n",
    "\n",
    "LOGGER = logging.getLogger(\"my_app\")\n",
    "\n",
    "LABELS = [\n",
    "    \"pants-fire\",\n",
    "    \"false\",\n",
    "    \"mostly-false\",\n",
    "    \"half-true\",\n",
    "    \"mostly-true\",\n",
    "    \"true\",\n",
    "]\n",
    "\n",
    "bf16_ok = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "fp16_ok = torch.cuda.is_available()\n",
    "dtype = torch.bfloat16 if bf16_ok else (torch.float16 if fp16_ok else torch.float32)\n",
    "\n",
    "\n",
    "def setup_logging() -> None:\n",
    "    config_file = Path(\"configs/config.json\")\n",
    "    if config_file.exists():\n",
    "        with config_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            config = json.load(f)\n",
    "        logging.config.dictConfig(config)\n",
    "    else:\n",
    "        logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "\n",
    "\n",
    "def _read_config(path: str | Path = \"configs/base.yaml\") -> dict[str, Any]:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Config file not found: {p}\")\n",
    "\n",
    "    try:\n",
    "        with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            cfg = yaml.safe_load(f) or {}\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Invalid YAML in {p}: {e}\") from e\n",
    "\n",
    "    cfg.setdefault(\"data\", {})\n",
    "    cfg.setdefault(\"model\", {})\n",
    "    cfg.setdefault(\"lora\", {})\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def load_model(cfg: dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Parameters for model, tokenizer, and QLoRa training.\n",
    "    Only a few parameters are added to configuration setting to spec as needed.\n",
    "    \"\"\"\n",
    "    LOGGER.info(\"Loading base model %s\", cfg[\"model\"].get(\"model_name\", \"<missing>\"))\n",
    "\n",
    "    base_model = \"Qwen/Qwen2.5-3B\"\n",
    "\n",
    "    # Qlora configuration - paper ref\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=dtype,\n",
    "    )\n",
    "\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model,\n",
    "        trust_remote_code=True,\n",
    "        dtype=dtype,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        low_cpu_mem_usage=True,\n",
    "        attn_implementation=\"sdpa\",\n",
    "    )\n",
    "\n",
    "    base.config.use_cache = False\n",
    "    base = prepare_model_for_kbit_training(base, use_gradient_checkpointing=True)\n",
    "\n",
    "    target_modules = [\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ]\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=target_modules,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        bias=\"none\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model, use_fast=True, trust_remote_code=True\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def build_tokenize_fn(tokenizer, prompt: str, max_length: int):\n",
    "    \"\"\"\n",
    "    Tokenize per-example, truncating the prompt to leave room for the target.\n",
    "    Returns variable-length lists; collator will handle padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize_batch(batch: dict[str, list[Any]]) -> dict[str, list[list[int]]]:\n",
    "        statements: list[str] = [str(s).strip() for s in batch[\"statement\"]]\n",
    "        verdicts: list[str] = [str(v).strip() for v in batch[\"verdict\"]]\n",
    "\n",
    "        input_ids_all: list[list[int]] = []\n",
    "        label_all: list[list[int]] = []\n",
    "\n",
    "        orig_side = tokenizer.truncation_side\n",
    "        tokenizer.truncation_side = \"right\"\n",
    "\n",
    "        for s, v in zip(statements, verdicts, strict=False):\n",
    "            text = prompt.format(statement=s)\n",
    "            target = \" \" + v\n",
    "\n",
    "            target_ids = tokenizer(target, add_special_tokens=False)[\"input_ids\"]\n",
    "            available = max(0, max_length - len(target_ids))\n",
    "            prompt_ids = tokenizer(\n",
    "                text,\n",
    "                add_special_tokens=False,\n",
    "                truncation=True,\n",
    "                max_length=available,\n",
    "            )[\"input_ids\"]\n",
    "\n",
    "            ids = prompt_ids + target_ids\n",
    "            lbl = ([-100] * len(prompt_ids)) + target_ids\n",
    "\n",
    "            input_ids_all.append(ids)\n",
    "            label_all.append(lbl)\n",
    "\n",
    "        tokenizer.truncation_side = orig_side\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids_all,\n",
    "            \"labels\": label_all,\n",
    "            \"verdict\": verdicts,\n",
    "            \"statement\": statements,\n",
    "        }\n",
    "\n",
    "    return tokenize_batch\n",
    "\n",
    "\n",
    "def data_processing(cfgs, tokenizer, split_name: str = \"training\"):\n",
    "    data_path = cfgs[\"data\"][split_name]\n",
    "    ds = load_dataset(\"json\",\n",
    "                      data_files=data_path,\n",
    "                      keep_in_memory=False,)\n",
    "\n",
    "    max_length = int(cfgs[\"data\"].get(\"max_length\", 1024))\n",
    "    tokenize_fn = build_tokenize_fn(\n",
    "        tokenizer, cfgs[\"prompt\"][\"template\"], max_length,\n",
    "        )\n",
    "\n",
    "    if split_name == \"training\":\n",
    "        split = ds[\"train\"].train_test_split(test_size=0.15, seed=cfgs.get(\"seed\", 7))\n",
    "        dataset = DatasetDict(\n",
    "            {\n",
    "                \"train\": split[\"train\"],\n",
    "                \"validation\": split[\"test\"],\n",
    "            }\n",
    "        )\n",
    "        cols_to_drop = dataset[\"train\"].column_names\n",
    "\n",
    "        def tokenize_for_train(batch: dict[str, list[Any]]) -> dict[str, Any]:\n",
    "            out = tokenize_fn(batch)\n",
    "            # We don't need these during training/validation; they only cost RAM.\n",
    "            out.pop(\"statement\", None)\n",
    "            out.pop(\"verdict\", None)\n",
    "            return out\n",
    "\n",
    "        return dataset.map(\n",
    "            tokenize_for_train,\n",
    "            batched=True,\n",
    "            batch_size=32,\n",
    "            num_proc=1,\n",
    "            remove_columns=cols_to_drop,\n",
    "            desc=\"Tokenizing (train/val)\",\n",
    "        )\n",
    "    else:\n",
    "        cols_to_drop = ds[\"train\"].column_names\n",
    "        # tokenize_fn returns \"statement\" and \"verdict\", so we can remove the original columns\n",
    "        # Access the \"train\" split before mapping since load_dataset returns DatasetDict with \"train\" key\n",
    "        tokenized = ds[\"train\"].map(\n",
    "            tokenize_fn,\n",
    "            batched=True,\n",
    "            batch_size=32,\n",
    "            num_proc=1,\n",
    "            remove_columns=cols_to_drop,\n",
    "            desc=f\"Tokenizing ({split_name})\",\n",
    "        )\n",
    "\n",
    "        return DatasetDict({split_name: tokenized})\n",
    "\n",
    "\n",
    "def decode_prediction_pairs(tokenizer, preds, labels) -> list[tuple[str, str]]:\n",
    "    \"\"\"Decode predicted/label token sequences into human-readable verdict strings.\"\"\"\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    pred_array = np.array(preds)\n",
    "    if pred_array.ndim == 3:\n",
    "        pred_ids = pred_array.argmax(-1)\n",
    "    else:\n",
    "        pred_ids = pred_array\n",
    "\n",
    "    label_ids = np.array(labels)\n",
    "    pred_ids = np.array(pred_ids)\n",
    "\n",
    "    decoded: list[tuple[str, str]] = []\n",
    "\n",
    "    for i in range(label_ids.shape[0]):\n",
    "        label_row = label_ids[i]\n",
    "        pred_row = pred_ids[i]\n",
    "\n",
    "        shifted_labels = label_row[1:]\n",
    "        shifted_preds = pred_row[:-1]\n",
    "\n",
    "        mask = shifted_labels != -100\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "\n",
    "        true_ids = shifted_labels[mask]\n",
    "        pred_span_ids = shifted_preds[mask]\n",
    "        true_text = tokenizer.decode(true_ids, skip_special_tokens=True).strip().lower()\n",
    "        pred_text = (\n",
    "            tokenizer.decode(pred_span_ids, skip_special_tokens=True).strip().lower()\n",
    "        )\n",
    "\n",
    "        decoded.append((true_text, pred_text))\n",
    "\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def build_metrics_computer(tokenizer):\n",
    "    \"\"\"\n",
    "    Exact-string accuracy for the target only. Convert from logits -> ids when needed and compare on the label positions (mask != -100).\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds, labels = eval_pred\n",
    "        decoded = decode_prediction_pairs(tokenizer, preds, labels)\n",
    "        y_true = [true for true, _ in decoded]\n",
    "        y_pred = [pred for _, pred in decoded]\n",
    "\n",
    "        accuracy = (\n",
    "            float((np.array(y_pred) == np.array(y_true)).mean()) if y_true else 0.0\n",
    "        )\n",
    "\n",
    "        valid_idx = [i for i, p in enumerate(y_pred) if p in LABELS]\n",
    "        if valid_idx:\n",
    "            vt = [y_true[i] for i in valid_idx]\n",
    "            vp = [y_pred[i] for i in valid_idx]\n",
    "\n",
    "            macro_f1 = f1_score(vt, vp, labels=LABELS, average=\"macro\", zero_division=0)\n",
    "            kappa = cohen_kappa_score(vt, vp, labels=LABELS)\n",
    "            recognized_ratio = len(valid_idx) / len(y_true)\n",
    "\n",
    "            prec, rec, f1, supp = precision_recall_fscore_support(\n",
    "                vt, vp, labels=LABELS, zero_division=0\n",
    "            )\n",
    "\n",
    "            metrics = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"macro_f1\": float(macro_f1),\n",
    "                \"cohen_kappa\": float(kappa),\n",
    "                \"recognized_ratio\": float(recognized_ratio),\n",
    "            }\n",
    "\n",
    "            for i, lab in enumerate(LABELS):\n",
    "                metrics[f\"per_class/precision/{lab}\"] = float(prec[i])\n",
    "                metrics[f\"per_class/recall/{lab}\"] = float(rec[i])\n",
    "                metrics[f\"per_class/f1/{lab}\"] = float(f1[i])\n",
    "                metrics[f\"per_class/support/{lab}\"] = float(supp[i])\n",
    "\n",
    "        else:\n",
    "            metrics = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"macro_f1\": 0.0,\n",
    "                \"cohen_kappa\": 0.0,\n",
    "                \"recognized_ratio\": 0.0,\n",
    "            }\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    return compute_metrics\n",
    "\n",
    "\n",
    "def write_json(metrics: dict[str, float], path: Path) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(metrics, file, indent=2)\n",
    "    LOGGER.info(\"Persisted metrics to %s\", path)\n",
    "\n",
    "\n",
    "def _manual_training_params() -> dict[str, Any]:\n",
    "    \"\"\"Single place to tweak core training loop knobs.\"\"\"\n",
    "\n",
    "    return {\n",
    "            \"output_dir\": Path(\"results/ar-qwen-mini\").resolve(),\n",
    "            \"logging_steps\": 50,   # how often to log training loss\n",
    "        }\n",
    "\n",
    "\n",
    "def train(model, tokenizer, cfgs: dict[str, Any], training_runtime: dict[str, Any]):\n",
    "\n",
    "    dataset = data_processing(cfgs, tokenizer, \"training\")\n",
    "    compute_metrics = build_metrics_computer(tokenizer)\n",
    "    collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer, label_pad_token_id=-100, pad_to_multiple_of=8\n",
    "    )\n",
    "\n",
    "    out_dir = training_runtime[\"output_dir\"]\n",
    "    log_every_steps = training_runtime[\"logging_steps\"]\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=str(out_dir),\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.1,\n",
    "\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        logging_steps=log_every_steps,\n",
    "        eval_accumulation_steps=1,\n",
    "\n",
    "        tf32=True,\n",
    "        bf16=bf16_ok,\n",
    "        fp16=(fp16_ok and not bf16_ok),\n",
    "        max_grad_norm=0.3,\n",
    "        dataloader_num_workers=0,\n",
    "        optim=\"adamw_bnb_8bit\",\n",
    "        save_safetensors=True,\n",
    "\n",
    "        report_to=[],\n",
    "\n",
    "        remove_unused_columns=True,\n",
    "        seed=cfgs[\"seed\"],\n",
    "\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        greater_is_better=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"],\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=collator,\n",
    "        args=args,\n",
    "        callbacks=[MemUsageCallback()],\n",
    "    )\n",
    "\n",
    "    trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))\n",
    "\n",
    "    LOGGER.info(\n",
    "        \"Starting training run (epochs=%s, lr=%s)\",\n",
    "        args.num_train_epochs,\n",
    "        args.learning_rate,\n",
    "    )\n",
    "\n",
    "    train_output = trainer.train()\n",
    "    train_metrics = train_output.metrics or {}\n",
    "    eval_metrics = trainer.evaluate()  # includes eval_loss + compute_metrics()\n",
    "    prediction_output = trainer.predict(dataset[\"validation\"])\n",
    "    decoded_pairs = decode_prediction_pairs(\n",
    "        tokenizer,\n",
    "        prediction_output.predictions,\n",
    "        prediction_output.label_ids,\n",
    "    )\n",
    "\n",
    "    numeric_metrics = {\n",
    "        k: float(v)\n",
    "        for k, v in {**train_metrics, **eval_metrics}.items()\n",
    "        if isinstance(v, (int, float))\n",
    "    }\n",
    "\n",
    "    if not numeric_metrics:\n",
    "        LOGGER.warning(\"No numeric metrics returned from trainer run.\")\n",
    "    else:\n",
    "        LOGGER.info(\"Final metrics (also written to training_metrics.json):\")\n",
    "        for key in sorted(numeric_metrics):\n",
    "            LOGGER.info(\"  %s = %.6f\", key, numeric_metrics[key])\n",
    "\n",
    "    write_json(numeric_metrics, out_dir / \"training_metrics.json\")\n",
    "\n",
    "    if decoded_pairs:\n",
    "        preview = decoded_pairs[: min(10, len(decoded_pairs))]\n",
    "        LOGGER.info(\"Sample validation predictions (predicted | actual):\")\n",
    "        for idx, (true_val, pred_val) in enumerate(preview, start=1):\n",
    "            LOGGER.info(\"  #%d %s | %s\", idx, pred_val, true_val)\n",
    "    else:\n",
    "        LOGGER.warning(\"No decoded predictions available for preview printing.\")\n",
    "\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--config\", default=\"configs/base.yaml\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    setup_logging()\n",
    "    LOGGER.info(\"Logging setup complete\")\n",
    "    CONFIG_DIR = Path(args.config).resolve()\n",
    "    cfgs = _read_config(CONFIG_DIR)\n",
    "    model, tokenizer = load_model(cfgs)\n",
    "    training_runtime = _manual_training_params()\n",
    "    train(model, tokenizer, cfgs, training_runtime)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
